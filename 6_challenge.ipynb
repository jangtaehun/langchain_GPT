{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.5)\n",
    "memory = ConversationBufferMemory(llm=llm, memory_key=\"history\", return_messages=True)\n",
    "\n",
    "\n",
    "loader = TextLoader(\"./document/chapter_three.txt\")\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100, \n",
    "    separators=\"\\n\",\n",
    ") \n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings) \n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ \n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "map_doc_chain = prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    # print(inputs)\n",
    "    documents = inputs['document']\n",
    "    question = inputs['question']\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": question\n",
    "    }).content for doc in documents)\n",
    "\n",
    "map_chain = {\"document\":retriever, \"question\":RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. 추가적으로 한글로 답해주세요.\n",
    "            ------\n",
    "            Here is the conversation history so far:\n",
    "            {history}\n",
    "            \n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_memory(_):\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    return history\n",
    "\n",
    "chain = {\"context\":map_chain, \"question\": RunnablePassthrough(), \"history\": RunnableLambda(get_memory) } | final_prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제가 알기로는 Aaronson이 유죄 판결을 받은 것은 아닌 것으로 알고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"Aaronson은 유죄인가요?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제가 알기로는 그가 테이블에 어떤 메시지를 쓴 것은 언급되지 않았습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"그가 테이블에 어떤 메시지를 썼나요?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그가 테이블에 쓴 메시지는 다음과 같습니다:\n",
      "\n",
      "FREEDOM IS SLAVERY\n",
      "\n",
      "TWO AND TWO MAKE FIVE\n",
      "\n",
      "GOD IS POWER\n",
      "\n",
      ":''\n",
      "\n",
      ": ''\n",
      "\n",
      ": ''\n"
     ]
    }
   ],
   "source": [
    "question = \"What message did he write in the table?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia는 위의 텍스트에서 언급된 캐릭터로, 주인공인 Winston Smith가 사랑하는 여성입니다. 위의 텍스트에서는 Winston이 감옥에서 살아가는 동안 Julia에 대한 강한 그리움과 사랑을 느낀 장면이 나타납니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"Julia 는 누구인가요?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/fp1_kg411bl0yfm4mg9jjycm0000gn/T/ipykernel_52039/1445792369.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"./document/chapter_three.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/fp1_kg411bl0yfm4mg9jjycm0000gn/T/ipykernel_52039/4264786169.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=20,\n",
    "    return_messages=True,\n",
    ")\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": load_memory,\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    print(result)\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='예, Aaronson은 유죄로 여겨졌습니다.' additional_kwargs={} response_metadata={'token_usage': <OpenAIObject at 0x1175a0e50> JSON: {\n",
      "  \"prompt_tokens\": 2586,\n",
      "  \"completion_tokens\": 21,\n",
      "  \"total_tokens\": 2607,\n",
      "  \"prompt_tokens_details\": {\n",
      "    \"cached_tokens\": 0,\n",
      "    \"audio_tokens\": 0\n",
      "  },\n",
      "  \"completion_tokens_details\": {\n",
      "    \"reasoning_tokens\": 0,\n",
      "    \"audio_tokens\": 0,\n",
      "    \"accepted_prediction_tokens\": 0,\n",
      "    \"rejected_prediction_tokens\": 0\n",
      "  }\n",
      "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-bd4d71ae-26aa-435f-b772-e2e06e7828f6-0'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson은 유죄인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='He wrote \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE\" on the table.' additional_kwargs={} response_metadata={'token_usage': <OpenAIObject at 0x117555d50> JSON: {\n",
      "  \"prompt_tokens\": 2664,\n",
      "  \"completion_tokens\": 27,\n",
      "  \"total_tokens\": 2691,\n",
      "  \"prompt_tokens_details\": {\n",
      "    \"cached_tokens\": 0,\n",
      "    \"audio_tokens\": 0\n",
      "  },\n",
      "  \"completion_tokens_details\": {\n",
      "    \"reasoning_tokens\": 0,\n",
      "    \"audio_tokens\": 0,\n",
      "    \"accepted_prediction_tokens\": 0,\n",
      "    \"rejected_prediction_tokens\": 0\n",
      "  }\n",
      "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-3b01972d-72c2-4ec2-944e-643fe561411f-0'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"그가 테이블에 어떤 메시지를 썼나요?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
