{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.5)\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "\n",
    "loader = TextLoader(\"./document/chapter_three.txt\")\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100, \n",
    "    separators=\"\\n\",\n",
    ") \n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings) \n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ \n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "map_doc_chain = prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    # print(inputs)\n",
    "    documents = inputs['document']\n",
    "    question = inputs['question']\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": question\n",
    "    }).content for doc in documents)\n",
    "\n",
    "map_chain = {\"document\":retriever, \"question\":RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. 추가적으로 한글로 답해주세요.\n",
    "            ------\n",
    "            Here is the conversation history so far:\n",
    "            {history}\n",
    "            \n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_memory(_):\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    return history\n",
    "\n",
    "chain = {\"context\":map_chain, \"question\": RunnablePassthrough(), \"history\": RunnableLambda(get_memory) } | final_prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaronson은 그의 혐의에 대해 유죄로 간주되었습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"Aaronson은 유죄인가요?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다, 그 정보는 제가 알고 있는 대로에 따르면 없습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"그가 테이블에 어떤 메시지를 썼나요?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia는 위의 텍스트에서 언급된 캐릭터로, 주인공인 Winston Smith가 사랑하는 여성입니다. 위의 텍스트에서는 Winston이 감옥에서 살아가는 동안 Julia에 대한 강한 그리움과 사랑을 느낀 장면이 나타납니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"Julia 는 누구인가요?\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전 질문은 \"Julia 는 누구인가요?\" 였습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"이전에 어떤 질문을 했나요\"\n",
    "result = chain.invoke(question)\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context({\"question\": question}, {\"response\": result.content})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
