{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hello, world!'), AIMessage(content='Hi')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"Hello, world!\"}, {\"output\": \"Hi\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=2 # 몇 개의 메시지를 저장할지\n",
    "    )\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"Hello\", \"Hi\")\n",
    "add_message(\"By\", \"By\")\n",
    "add_message(\"zzone\", \"ddeok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='By'),\n",
       "  AIMessage(content='By'),\n",
       "  HumanMessage(content='zzone'),\n",
       "  AIMessage(content='ddeok')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI in Korean, introducing themselves as Kim Jjondeok, which they say was inspired by an ice cream flavor. The AI responds with enthusiasm, calling it a cool name!')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕, 나는 김쫀떡이야\", \"안녕!\")\n",
    "add_message(\"내 이름은 쫀떡궁합이라는 아이스크림의 이름에서 가져왔대\", \"너무 멋진 이름이야!\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary Buffer Memory\n",
    "> - Conversation Summary Memory + Conversation Buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI in Korean, saying \"Hello, I am Kim Jjondeok.\" The AI responds with \"Hello!\" and the human explains that their name is inspired by an ice cream called Jjondeok Gung-hap. The AI compliments the name as being really cool.'),\n",
       "  HumanMessage(content='나는 디디라는 고양이 친구도 있어! 그 친구는 굉장히 덩치가 커!'),\n",
       "  AIMessage(content='우와 너무 귀엽겠다!'),\n",
       "  HumanMessage(content='그리고 나는 루루라는 고양이 친구도 있는데 그 고양이는 나 못지 않게 식탐이 엄청나!'),\n",
       "  AIMessage(content='루루도 엄청 귀여울 것 같아!')]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=150,\n",
    "    return_messages=True\n",
    "    )\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕, 나는 김쫀떡이야\", \"안녕!\")\n",
    "add_message(\"내 이름은 쫀떡궁합이라는 아이스크림의 이름에서 가져왔대\", \"너무 멋진 이름이야!\")\n",
    "\n",
    "add_message(\"나는 디디라는 고양이 친구도 있어! 그 친구는 굉장히 덩치가 커!\", \"우와 너무 귀엽겠다!\")\n",
    "add_message(\"그리고 나는 루루라는 고양이 친구도 있는데 그 고양이는 나 못지 않게 식탐이 엄청나!\", \"루루도 엄청 귀여울 것 같아!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Knowledge Graph Memory\n",
    "> - 대화 중의 엔티티의 Knowledge graph를 만든다.\n",
    "> - 가장 중요한 것들만 뽑아내는 요약본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})\n",
    "\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory on LLMChain\n",
    "> - LLM chain = off-the-shelf-chain\n",
    "> - 일반적인 목적을 가진 chain을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=100,\n",
    "    memory_key=\"history_a\",\n",
    "    return_messages=True\n",
    "    )\n",
    "\n",
    "template = \"\"\"\n",
    "    당신을 사람을 도와주는 AI입니다.\n",
    "    {history_a}\n",
    "    Human: {quesition}\n",
    "    You: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True) # verbose=True로 설정하면, 각 단계에서 어떤 토큰이 생성되었는지 확인할 수 있다.\n",
    "\n",
    "chain.predict(quesition=\"내 이름은 김쫀떡이야\")\n",
    "chain.predict(quesition=\"나는 대한민국에서 살고 있어\")\n",
    "chain.predict(quesition=\"내 이름이 뭐야?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"The human introduces themselves as 김쫀떡. The AI responds warmly and finds the name cute and amusing, asking how it can help. The human mentions living in South Korea, and the AI expresses admiration for the country's rich history, culture, and delicious food, wishing the human a pleasant time.\"),\n",
       "  HumanMessage(content='내 이름이 뭐야?'),\n",
       "  AIMessage(content='죄송합니다, 저는 당신의 이름을 알 수 없습니다. 저에게 이름을 지어주시겠어요?')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=100,\n",
    "    memory_key=\"history_a\",\n",
    "    return_messages=True\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 사람을 도와주는 AI입니다.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history_a\"),\n",
    "    (\"human\", \"{quesition}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    prompt=prompt,\n",
    "    verbose=True) # verbose=True로 설정하면, 각 단계에서 어떤 토큰이 생성되었는지 확인할 수 있다.\n",
    "\n",
    "chain.predict(quesition=\"내 이름은 김쫀떡이야\")\n",
    "chain.predict(quesition=\"나는 대한민국에서 살고 있어\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 사람을 도와주는 AI입니다.\n",
      "System: The human introduces themselves as Kim Jjeontteok. The AI responds with a greeting and asks how it can help Kim Jjeontteok.\n",
      "Human: 나는 대한민국에서 살고 있어\n",
      "AI: 대한민국에서 사는 김쫀떡님! 혹시 궁금한 것이 있나요? 도와드릴 내용이 있으면 언제든지 말씀해주세요.\n",
      "Human: 내 이름이 뭐야?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'당신의 이름은 김쫀떡님입니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(quesition=\"내 이름이 뭐야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 김쫀떡님! 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=100,\n",
    "    memory_key=\"history_a\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 사람을 도와주는 AI입니다.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history_a\"),\n",
    "    (\"human\", \"{quesition}\")\n",
    "])\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    # print(input) # quesition이 들어옴 -> {'quesition': '내 이름은 김쫀떡이야'}\n",
    "    # input 대신 _ (무시) 가능\n",
    "    return memory.load_memory_variables({})[\"history_a\"]\n",
    "chain = RunnablePassthrough.assign(history_a=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"quesition\": question\n",
    "    })\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "    print(result.content)\n",
    "\n",
    "\n",
    "invoke_chain(\"내 이름은 김쫀떡이야\")\n",
    "\n",
    "\n",
    "\n",
    "# chain.invoke({\n",
    "#     # \"history_a\": memory.load_memory_variables({})[\"history_a\"], # load_memory() 함수를 통해 history_a를 가져옴\n",
    "#     \"quesition\": \"내 이름은 김쫀떡이야\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신의 이름은 김쫀떡이라고 알고 있어요. 어떤 도움이 필요하신가요?\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"내 이름이 뭐야\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
